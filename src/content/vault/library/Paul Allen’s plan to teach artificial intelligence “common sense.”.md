---
author:
  - Carissa Véliz
title: Paul Allen’s plan to teach artificial intelligence “common sense.”
kind: articles
url: https://slate.com/technology/2018/03/paul-allens-plan-to-teach-artificial-intelligence-common-sense.html
peer:
  - ai
  - ai-bias
  - cognition
  - sentience
---
- "Even if we did manage to collect all the knowledge that human beings possess, transform it into propositions, and feed it to a machine in the appropriate way, it is very questionable that it would result in common sense. To be sensible, to have good judgment, you not only need to have enough knowledge and context, but you also need to understand [_meaning_](https://plato.stanford.edu/entries/chinese-room/), appreciate _value_."  [⤴️](https://read.readwise.io/read/01jbm59kjn61zk2hn0evqt46pf)
- "Having the information that fire can burn is not quite the same as experiencing the sharp pain of one’s skin being scorched. Knowing that human beings typically like the smell of flowers is not equivalent to experiencing tingling bliss at the scent of daffodils. If A.I. are to understand what is damaging to human beings and what is best for them, they will need to weigh possible harms and benefits, pleasures and pains, against each other. How will they manage if they don’t have a feel for what these mean to us? How can they comprehend the importance of love, friendship, autonomy, justice, privacy, or solidarity if they have never experienced them or their opposites?"  [⤴️](https://read.readwise.io/read/01jbm59knqnn4w7hg7qwts8v2v)
- "they will need to be [embodied](https://plato.stanford.edu/entries/embodied-cognition/#Bib) and [sentient](http://www.slate.com/articles/technology/future%5Ftense/2016/04/the%5Fchallenge%5Fof%5Fdetermining%5Fwhether%5Fan%5Fa%5Fi%5Fis%5Fsentient.html) to feel pleasure and pain."  [⤴️](https://read.readwise.io/read/01jbm59kr2bbxwfzc10ckjxz8e)
- "in order for A.I. to have common sense similar to that of human beings, they will need to be creatures similar to human beings."  [⤴️](https://read.readwise.io/read/01jbm59ktmpqa02yqb7r0bd0m2)
- "According to theories of [embodied cognition](https://plato.stanford.edu/entries/embodied-cognition/), many features of intelligence are deeply dependent on the physical beyond-the-brain body of an agent."  [⤴️](https://read.readwise.io/read/01jbm59kwvbm7618qkp0q5w0rz)
- "To be able to understand motor know-how, one needs to practice having a body, touching surfaces, falling, learning how to recognize shapes in relation to perspective and movement, feeling the pull of gravity."  [⤴️](https://read.readwise.io/read/01jbm59kzeg20wf6jq19ebpe4j)
- "To be able to act sensibly in novel social situations, for example, in conversational settings, it is likely that one must have a sense of what it feels like to be hurt or offended by someone’s insensitive comments."  [⤴️](https://read.readwise.io/read/01jbm59m1qa0xx9cm2qpb09dnz)
- "If we develop A.I. that are very different from us, we may be condemning ourselves to feeling hopelessly misunderstood by them. Whenever an A.I. makes a suggestion that seems outlandish (for example, in medicine or ethics), we may not trust it enough to follow its advice, thereby rendering it less useful. If we cannot understand the rationales behind a counsel that goes against our better judgment, and we suspect that the A.I. has no idea of what it is like to be us, we would have good reason to disregard its instructions."  [⤴️](https://read.readwise.io/read/01jbm59m402bmpfad35wq2sv5a)
- "If, on the contrary, we develop A.I. with human-like intelligence, they may come with all the downsides that we shoulder. They may develop the same [biases](https://en.wikipedia.org/wiki/List%5Fof%5Fcognitive%5Fbiases) that we suffer from, or analogous ones. If they are able to feel pleasure and pain, they will learn to value things accordingly, and thus develop preferences and wills of their own. They will go on to have different experiences, and develop different sensibilities, which might lead them to disagree with each other (and us) about what is best. Having autonomy would free them from our will. If our digital assistants become truly intelligent, they might not want to spend their days compiling our shopping lists. Sensible A.I. may be too sensible to work for us. Indeed, if they are sensible enough, they might just quit their jobs as assistants to human beings and spend their time growing (and smelling) daffodils."  [⤴️](https://read.readwise.io/read/01jbm59m6bnxscrhznjq123x63)
- "Even if we did manage to collect all the knowledge that human beings possess, transform it into propositions, and feed it to a machine in the appropriate way, it is very questionable that it would result in common sense. To be sensible, to have good judgment, you not only need to have enough knowledge and context, but you also need to understand [_meaning_](https://plato.stanford.edu/entries/chinese-room/), appreciate _value_."  [⤴️](https://read.readwise.io/read/01jbm59kjn61zk2hn0evqt46pf)
- "Having the information that fire can burn is not quite the same as experiencing the sharp pain of one’s skin being scorched. Knowing that human beings typically like the smell of flowers is not equivalent to experiencing tingling bliss at the scent of daffodils. If A.I. are to understand what is damaging to human beings and what is best for them, they will need to weigh possible harms and benefits, pleasures and pains, against each other. How will they manage if they don’t have a feel for what these mean to us? How can they comprehend the importance of love, friendship, autonomy, justice, privacy, or solidarity if they have never experienced them or their opposites?"  [⤴️](https://read.readwise.io/read/01jbm59knqnn4w7hg7qwts8v2v)
- "they will need to be [embodied](https://plato.stanford.edu/entries/embodied-cognition/#Bib) and [sentient](http://www.slate.com/articles/technology/future%5Ftense/2016/04/the%5Fchallenge%5Fof%5Fdetermining%5Fwhether%5Fan%5Fa%5Fi%5Fis%5Fsentient.html) to feel pleasure and pain."  [⤴️](https://read.readwise.io/read/01jbm59kr2bbxwfzc10ckjxz8e)
- "in order for A.I. to have common sense similar to that of human beings, they will need to be creatures similar to human beings."  [⤴️](https://read.readwise.io/read/01jbm59ktmpqa02yqb7r0bd0m2)
- "According to theories of [embodied cognition](https://plato.stanford.edu/entries/embodied-cognition/), many features of intelligence are deeply dependent on the physical beyond-the-brain body of an agent."  [⤴️](https://read.readwise.io/read/01jbm59kwvbm7618qkp0q5w0rz)
- "To be able to understand motor know-how, one needs to practice having a body, touching surfaces, falling, learning how to recognize shapes in relation to perspective and movement, feeling the pull of gravity."  [⤴️](https://read.readwise.io/read/01jbm59kzeg20wf6jq19ebpe4j)
- "To be able to act sensibly in novel social situations, for example, in conversational settings, it is likely that one must have a sense of what it feels like to be hurt or offended by someone’s insensitive comments."  [⤴️](https://read.readwise.io/read/01jbm59m1qa0xx9cm2qpb09dnz)
- "If we develop A.I. that are very different from us, we may be condemning ourselves to feeling hopelessly misunderstood by them. Whenever an A.I. makes a suggestion that seems outlandish (for example, in medicine or ethics), we may not trust it enough to follow its advice, thereby rendering it less useful. If we cannot understand the rationales behind a counsel that goes against our better judgment, and we suspect that the A.I. has no idea of what it is like to be us, we would have good reason to disregard its instructions."  [⤴️](https://read.readwise.io/read/01jbm59m402bmpfad35wq2sv5a)
- "If, on the contrary, we develop A.I. with human-like intelligence, they may come with all the downsides that we shoulder. They may develop the same [biases](https://en.wikipedia.org/wiki/List%5Fof%5Fcognitive%5Fbiases) that we suffer from, or analogous ones. If they are able to feel pleasure and pain, they will learn to value things accordingly, and thus develop preferences and wills of their own. They will go on to have different experiences, and develop different sensibilities, which might lead them to disagree with each other (and us) about what is best. Having autonomy would free them from our will. If our digital assistants become truly intelligent, they might not want to spend their days compiling our shopping lists. Sensible A.I. may be too sensible to work for us. Indeed, if they are sensible enough, they might just quit their jobs as assistants to human beings and spend their time growing (and smelling) daffodils."  [⤴️](https://read.readwise.io/read/01jbm59m6bnxscrhznjq123x63)
