---
title: TfL's AI Tube Station experiment is amazing and slightly terrifying
editUrl: false
author:
  - James O'Malley
summary: Mind the Orwellian Surveillance Apparatus
kind: articles
url: https://takes.jamesomalley.co.uk/p/tfls-ai-tube-station-experiment-is
published_date: 2024-02-12
peer:
  - ai
  - ai-ethics
  - privacy
  - surveillance
  - tech-ethics
  - tech-safety
---

* "the system could apparently identify up to 77 different ‘use cases’ – though only eleven were used during trial. This ranges from significant incidents, like fare evasion, crime and anti-social behaviour, all the way down to more trivial matters, like spilled drinks or even discarded newspapers."  [⤴️](https://read.readwise.io/read/01jbm55zpc11avjtqwewbaptpk)
* "in the “safeguarding” bucket of use-cases, the AI was programmed to alert staff if a person was sat on a bench for longer than ten minutes or if they were in the ticket hall for longer than 15 minutes, as it implies they may be lost or require help."  [⤴️](https://read.readwise.io/read/01jbm55zsswy42pkesnxk4mq6x)
* "They instead trained the system to spot people with both arms raised in the air – because this is thought to be a “common behaviour” linked to acts of aggression"  [⤴️](https://read.readwise.io/read/01jbm55zw284eqr3q9k0amrjvr)
* "attempts by the AI to spot “aggressive behaviour”."  [⤴️](https://read.readwise.io/read/01jbm55zyetd7rm3293aa7dgsd)
* "if they are faced with someone behaving violently, and are unable to reach their radio to call for help, the *staff themselves* can simply raise their arms to trigger an alert to their colleagues."  [⤴️](https://read.readwise.io/read/01jbm5600sq81kxpfqszt7hnvd)
* "TfL manually scrubbed through and tagged “several hours” of CCTV footage to train the system to teach it what fare evasion looks like."  [⤴️](https://read.readwise.io/read/01jbm56033aetazq4075yxc0rz)
* "TfL had to go back and reconfigure it to avoid counting kids as fare evaders – and they did it by automatically disregarding anyone shorter than the ticket gates."  [⤴️](https://read.readwise.io/read/01jbm56061mhpbtmgyn21s2c3j)
* "It could be possible for augmented reality software to guide cleaning and maintenance staff to the spill that needs mopping up or the lightbulb that needs changing. Or when you plan a journey on Google Maps, it could warn you to avoid changing at Stratford to avoid the West Ham fans upset after a six-nil loss"  [⤴️](https://read.readwise.io/read/01jbm5608cqk8dfy4dxnx94pea)
* "Need to build a business case to install a lift? Now you can get an exact count of the number of people passing through with wheelchairs, prams and over-sized luggage."  [⤴️](https://read.readwise.io/read/01jbm560axdkdgd70zwh6hd4dg)
* "It would be trivial from a software (if not legal) perspective to train the cameras to identify, say, Israeli or Palestinian flags – or any other symbol you don’t like. The system could be used to surveil staff, and work them even harder, by literally keeping a by-the-second count of their idle time while on shift. And of course, the black-box AI training data could turn out the be flawed, perhaps unfairly or disproportionately identifying fare evaders with certain skin colours."  [⤴️](https://read.readwise.io/read/01jbm560dh4khgyvswej6fegbp)
* "So this is just a thing that exists in the world now. And even if we wanted to stop it being used, to do so would be just as *virtually impossible* as inventing it felt in the first place."  [⤴️](https://read.readwise.io/read/01jbm560g1dbftqv3zb31vryvv)
* "the system could apparently identify up to 77 different ‘use cases’ – though only eleven were used during trial. This ranges from significant incidents, like fare evasion, crime and anti-social behaviour, all the way down to more trivial matters, like spilled drinks or even discarded newspapers."  [⤴️](https://read.readwise.io/read/01jbm55zpc11avjtqwewbaptpk)
* "in the “safeguarding” bucket of use-cases, the AI was programmed to alert staff if a person was sat on a bench for longer than ten minutes or if they were in the ticket hall for longer than 15 minutes, as it implies they may be lost or require help."  [⤴️](https://read.readwise.io/read/01jbm55zsswy42pkesnxk4mq6x)
* "They instead trained the system to spot people with both arms raised in the air – because this is thought to be a “common behaviour” linked to acts of aggression"  [⤴️](https://read.readwise.io/read/01jbm55zw284eqr3q9k0amrjvr)
* "attempts by the AI to spot “aggressive behaviour”."  [⤴️](https://read.readwise.io/read/01jbm55zyetd7rm3293aa7dgsd)
* "if they are faced with someone behaving violently, and are unable to reach their radio to call for help, the *staff themselves* can simply raise their arms to trigger an alert to their colleagues."  [⤴️](https://read.readwise.io/read/01jbm5600sq81kxpfqszt7hnvd)
* "TfL manually scrubbed through and tagged “several hours” of CCTV footage to train the system to teach it what fare evasion looks like."  [⤴️](https://read.readwise.io/read/01jbm56033aetazq4075yxc0rz)
* "TfL had to go back and reconfigure it to avoid counting kids as fare evaders – and they did it by automatically disregarding anyone shorter than the ticket gates."  [⤴️](https://read.readwise.io/read/01jbm56061mhpbtmgyn21s2c3j)
* "It could be possible for augmented reality software to guide cleaning and maintenance staff to the spill that needs mopping up or the lightbulb that needs changing. Or when you plan a journey on Google Maps, it could warn you to avoid changing at Stratford to avoid the West Ham fans upset after a six-nil loss"  [⤴️](https://read.readwise.io/read/01jbm5608cqk8dfy4dxnx94pea)
* "Need to build a business case to install a lift? Now you can get an exact count of the number of people passing through with wheelchairs, prams and over-sized luggage."  [⤴️](https://read.readwise.io/read/01jbm560axdkdgd70zwh6hd4dg)
* "It would be trivial from a software (if not legal) perspective to train the cameras to identify, say, Israeli or Palestinian flags – or any other symbol you don’t like. The system could be used to surveil staff, and work them even harder, by literally keeping a by-the-second count of their idle time while on shift. And of course, the black-box AI training data could turn out the be flawed, perhaps unfairly or disproportionately identifying fare evaders with certain skin colours."  [⤴️](https://read.readwise.io/read/01jbm560dh4khgyvswej6fegbp)
* "So this is just a thing that exists in the world now. And even if we wanted to stop it being used, to do so would be just as *virtually impossible* as inventing it felt in the first place."  [⤴️](https://read.readwise.io/read/01jbm560g1dbftqv3zb31vryvv)
